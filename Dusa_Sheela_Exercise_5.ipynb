{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 5**\n",
        "\n",
        "**This exercise aims to provide a comprehensive learning experience in text analysis and machine learning techniques, focusing on both text classification and clustering tasks.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks***.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission, and no requests will be answered. Manage your time accordingly.**\n"
      ],
      "metadata": {
        "id": "TU-pLW33lpcS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## **Question 1 (20 Points)**\n",
        "\n",
        "The purpose of the question is to practice different machine learning algorithms for **text classification** as well as the performance evaluation. In addition, you are requried to conduct **10 fold cross validation** (https://scikit-learn.org/stable/modules/cross_validation.html) in the training.\n",
        "\n",
        "\n",
        "\n",
        "The dataset can be download from canvas. The dataset contains two files train data and test data for sentiment analysis in IMDB review, it has two categories: 1 represents positive and 0 represents negative. You need to split the training data into training and validate data (80% for training and 20% for validation, https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6) and perform 10 fold cross validation while training the classifier. The final trained model was final evaluated on the test data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algorithms:**\n",
        "\n",
        "*   MultinominalNB\n",
        "*   SVM\n",
        "*   KNN\n",
        "*   Decision tree\n",
        "*   Random Forest\n",
        "*   XGBoost\n",
        "*   Word2Vec\n",
        "*   BERT\n",
        "\n",
        "**Evaluation measurement:**\n",
        "\n",
        "\n",
        "*   Accuracy\n",
        "*   Recall\n",
        "*   Precison\n",
        "*   F-1 score\n"
      ],
      "metadata": {
        "id": "loi8Sh7UE6ha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "df = pd.read_csv(\"stsa_train.csv\")\n",
        "\n",
        "# Display the shape of the DataFrame\n",
        "print(\"Shape of the DataFrame:\")\n",
        "print(df.shape)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-4XzfCCGw3K",
        "outputId": "2273aca9-c2e3-4b65-a04e-365c0ae2d6f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the DataFrame:\n",
            "(6920, 2)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few rows of the DataFrame\n",
        "print(\"First few rows of the DataFrame:\")\n",
        "print(df.head())\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igah-BCZI9R9",
        "outputId": "22c397ff-9d77-463f-dbb5-bd593f7735ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First few rows of the DataFrame:\n",
            "   sentiment                                             review\n",
            "0          1  a stirring , funny and finally transporting re...\n",
            "1          0  apparently reassembled from the cutting-room f...\n",
            "2          0  they presume their audience wo n't sit still f...\n",
            "3          1  this is a visually stunning rumination on love...\n",
            "4          1  jonathan parker 's bartleby should have been t...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display summary statistics of the DataFrame\n",
        "print(\"Summary statistics of the DataFrame:\")\n",
        "print(df.describe())\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSXMYuQSI_tI",
        "outputId": "1a52aeb0-f01e-4298-980c-319ea6025bd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary statistics of the DataFrame:\n",
            "         sentiment\n",
            "count  6920.000000\n",
            "mean      0.521676\n",
            "std       0.499566\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       1.000000\n",
            "75%       1.000000\n",
            "max       1.000000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "data = pd.read_csv(\"stsa_test.csv\")\n",
        "\n",
        "# Display the shape of the DataFrame\n",
        "print(\"Shape of the DataFrame:\")\n",
        "print(data.shape)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHDg2DmbJCVx",
        "outputId": "09d0ac39-3c14-46c0-faf3-d2ac5a9b4939"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the DataFrame:\n",
            "(1821, 2)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few rows of the DataFrame\n",
        "print(\"First few rows of the DataFrame:\")\n",
        "print(data.head())\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6Uz6zJxJP4u",
        "outputId": "90fcb775-20d6-4054-a4e3-5a1ba66241a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First few rows of the DataFrame:\n",
            "   sentiment                                             review\n",
            "0          0     no movement , no yuks , not much of anything .\n",
            "1          0  a gob of drivel so sickly sweet , even the eag...\n",
            "2          0  gangs of new york is an unapologetic mess , wh...\n",
            "3          0  we never really feel involved with the story ,...\n",
            "4          1            this is one of polanski 's best films .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display summary statistics of the DataFrame\n",
        "print(\"Summary statistics of the DataFrame:\")\n",
        "print(data.describe())\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaP060rTJQTO",
        "outputId": "af1a64ba-aab9-4653-ee07-68262c6c85ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary statistics of the DataFrame:\n",
            "         sentiment\n",
            "count  1821.000000\n",
            "mean      0.499176\n",
            "std       0.500137\n",
            "min       0.000000\n",
            "25%       0.000000\n",
            "50%       0.000000\n",
            "75%       1.000000\n",
            "max       1.000000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Loading the training and testing datasets\n",
        "data_one = pd.read_csv('stsa_train.csv')\n",
        "data_two = pd.read_csv('stsa_test.csv')\n",
        "\n",
        "# Separating features (reviews) and labels (sentiments)\n",
        "X_one = data_one['review']\n",
        "y_one = data_one['sentiment']\n",
        "X_two = data_two['review']\n",
        "y_two = data_two['sentiment']\n",
        "\n",
        "# Splitting the training data for validation (80-20 split)\n",
        "X_one, X_three, y_one, y_three = train_test_split(X_one, y_one, test_size=0.2, random_state=42)\n",
        "\n",
        "# Defining a set of classifiers\n",
        "models = {\n",
        "    \"Classifier1\": MultinomialNB(),\n",
        "    \"Classifier2\": SVC(),\n",
        "    \"Classifier3\": KNeighborsClassifier(),\n",
        "    \"Classifier4\": DecisionTreeClassifier(),\n",
        "    \"Classifier5\": RandomForestClassifier(),\n",
        "    \"Classifier6\": XGBClassifier()\n",
        "}\n",
        "\n",
        "# Defining evaluation metrics\n",
        "metrics = {\n",
        "    \"Accuracy\": accuracy_score,\n",
        "    \"Precision\": precision_score,\n",
        "    \"Recall_Score\": recall_score,\n",
        "    \"F1_Score\": f1_score\n",
        "}\n",
        "\n",
        "\n",
        "# Performing 10-fold cross-validation for each classifier\n",
        "for model_name, classifier in models.items():\n",
        "    pipeline = make_pipeline(CountVectorizer(), classifier)\n",
        "    cv_scores = cross_val_score(pipeline, X_one, y_one, cv=10)\n",
        "    print(f\"\\n{model_name}:\")\n",
        "    print(\"Cross-validation scores:\", cv_scores)\n",
        "    print(\"Mean Cross-validation score:\", cv_scores.mean())\n",
        "\n",
        "    # Training the final model\n",
        "    pipeline.fit(X_one, y_one)\n",
        "\n",
        "    # Evaluating on the validation set\n",
        "    y_three_pred = pipeline.predict(X_three)\n",
        "    print(\"Validation Metrics:\")\n",
        "    for metric_name, metric_function in metrics.items():\n",
        "        metric_value = metric_function(y_three, y_three_pred)\n",
        "        print(f\"{metric_name}: {metric_value}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59Pw8Sv7aUgM",
        "outputId": "c6fae4aa-8550-4978-c3fd-bab2acee3140"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classifier1:\n",
            "Cross-validation scores: [0.75631769 0.79783394 0.78519856 0.79061372 0.79061372 0.76173285\n",
            " 0.78481013 0.78481013 0.75406872 0.78481013]\n",
            "Mean Cross-validation score: 0.7790809565154947\n",
            "Validation Metrics:\n",
            "Accuracy: 0.7947976878612717\n",
            "Precision: 0.777490297542044\n",
            "Recall_Score: 0.8429172510518934\n",
            "F1_Score: 0.8088829071332435\n",
            "\n",
            "Classifier2:\n",
            "Cross-validation scores: [0.73465704 0.72382671 0.71299639 0.72021661 0.74368231 0.76173285\n",
            " 0.7522604  0.72151899 0.71971067 0.75768535]\n",
            "Mean Cross-validation score: 0.734828732022901\n",
            "Validation Metrics:\n",
            "Accuracy: 0.7557803468208093\n",
            "Precision: 0.7394636015325671\n",
            "Recall_Score: 0.8120617110799438\n",
            "F1_Score: 0.7740641711229947\n",
            "\n",
            "Classifier3:\n",
            "Cross-validation scores: [0.58483755 0.56137184 0.58844765 0.57581227 0.5631769  0.55234657\n",
            " 0.60036166 0.56600362 0.54972875 0.54068716]\n",
            "Mean Cross-validation score: 0.5682773973273447\n",
            "Validation Metrics:\n",
            "Accuracy: 0.6163294797687862\n",
            "Precision: 0.6226415094339622\n",
            "Recall_Score: 0.6479663394109397\n",
            "F1_Score: 0.6350515463917525\n",
            "\n",
            "Classifier4:\n",
            "Cross-validation scores: [0.63718412 0.6299639  0.62093863 0.61371841 0.64259928 0.61913357\n",
            " 0.64918626 0.62206148 0.60759494 0.61663653]\n",
            "Mean Cross-validation score: 0.6259017110477149\n",
            "Validation Metrics:\n",
            "Accuracy: 0.6372832369942196\n",
            "Precision: 0.6379084967320261\n",
            "Recall_Score: 0.6844319775596073\n",
            "F1_Score: 0.6603518267929634\n",
            "\n",
            "Classifier5:\n",
            "Cross-validation scores: [0.69855596 0.70758123 0.69314079 0.69314079 0.74729242 0.75270758\n",
            " 0.74502712 0.71066908 0.69620253 0.76311031]\n",
            "Mean Cross-validation score: 0.7207427814154497\n",
            "Validation Metrics:\n",
            "Accuracy: 0.740606936416185\n",
            "Precision: 0.7223618090452262\n",
            "Recall_Score: 0.8064516129032258\n",
            "F1_Score: 0.7620941020543407\n",
            "\n",
            "Classifier6:\n",
            "Cross-validation scores: [0.67870036 0.68772563 0.7166065  0.66967509 0.73465704 0.73104693\n",
            " 0.73056058 0.70524412 0.7124774  0.73417722]\n",
            "Mean Cross-validation score: 0.710087086518563\n",
            "Validation Metrics:\n",
            "Accuracy: 0.7398843930635838\n",
            "Precision: 0.7129071170084439\n",
            "Recall_Score: 0.8288920056100981\n",
            "F1_Score: 0.7665369649805446\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the trained models on the test dataset\n",
        "print(\"\\nFinal Evaluation on Test Data:\")\n",
        "for model_name, classifier in models.items():\n",
        "    pipeline = make_pipeline(CountVectorizer(), classifier)\n",
        "    pipeline.fit(X_one, y_one)\n",
        "    y_two_pred = pipeline.predict(X_two)\n",
        "    print(f\"\\n{model_name}:\")\n",
        "    for metric_name, metric_function in metrics.items():\n",
        "        metric_value = metric_function(y_two, y_two_pred)\n",
        "        print(f\"{metric_name}: {metric_value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkAEFf18PKUA",
        "outputId": "426eb50c-a667-4b5f-ffdd-e8b542a03835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Evaluation on Test Data:\n",
            "\n",
            "Classifier1:\n",
            "Accuracy: 0.8072487644151565\n",
            "Precision: 0.7852760736196319\n",
            "Recall_Score: 0.8448844884488449\n",
            "F1_Score: 0.8139904610492847\n",
            "\n",
            "Classifier2:\n",
            "Accuracy: 0.757276221856123\n",
            "Precision: 0.7346733668341708\n",
            "Recall_Score: 0.8041804180418042\n",
            "F1_Score: 0.7678571428571429\n",
            "\n",
            "Classifier3:\n",
            "Accuracy: 0.5831960461285008\n",
            "Precision: 0.5773195876288659\n",
            "Recall_Score: 0.6160616061606161\n",
            "F1_Score: 0.5960617349654072\n",
            "\n",
            "Classifier4:\n",
            "Accuracy: 0.6392092257001647\n",
            "Precision: 0.6293634496919918\n",
            "Recall_Score: 0.6743674367436744\n",
            "F1_Score: 0.6510886882634095\n",
            "\n",
            "Classifier5:\n",
            "Accuracy: 0.7243272926963207\n",
            "Precision: 0.7024875621890547\n",
            "Recall_Score: 0.7766776677667767\n",
            "F1_Score: 0.7377220480668758\n",
            "\n",
            "Classifier6:\n",
            "Accuracy: 0.727622185612301\n",
            "Precision: 0.6938967136150235\n",
            "Recall_Score: 0.812981298129813\n",
            "F1_Score: 0.7487335359675785\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multinomial Naive Bayes (Classifier1): With validation and tests well approximated, the data reveals a good balance on recall and precision which is about 0.8. This balance is appropriate.\n",
        "\n",
        "Support Vector Machine (Classifier2): Effectiveness decreases as compared to Naive Bayes while dealing with either datasets. It resembles Naive Bayes highly accurate approach but is not as detailed, therefore negative reviews can be mistaken positive as well.\n",
        "\n",
        "K-Nearest Neighbors (Classifier3): The lowest accuracy of all the models among the subject models. It fails to discern cheerful or sad people.\n",
        "\n",
        "Decision Tree (Classifier4): Although KNN has marginally higher accuracy it still seems to be not highly relevant. Just like KNN, it will probably have other issues related to sentiment classification.\n",
        "\n",
        "Random Forest (Classifier5): Top among the others like KNN and DT with moderate accuracy. It is best in high rates of recalling positive reviews yet Naive Bayes has a better precision rate than it.\n",
        "\n",
        "XGBoost (Classifier6): Raising an almost similar level accuracy of the Random Forest but with only a mild hesitation for positive reviews in the test set. Contrary to it, it possesses the least reliable accuracy what means that sometimes, it may mistakenly mark certain reviews as negative.\n",
        "\n",
        "Overall:\n",
        "\n",
        "\n",
        "A multinomial bayes naive was shown to be the best of others in the accuracy and the precision with a slight drop in the recall.\n",
        "\n",
        "A Random Forest or XGBoost can be good alternatives, particularly if you need to have a high recall while identifying positive reviews. Nevertheless, such outcome may consist in the missed labeling of several negative reviews.\n",
        "\n",
        "The Support Vector Machine can, perhaps, be better suitable when accuracy is the main target, but it performs less.\n",
        "\n",
        "KNN and Decision Tree were the least performers here and they can probably be eliminated from the model."
      ],
      "metadata": {
        "id": "2RB3tG5R5Sv7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## **Question 2 (20 Points)**\n",
        "\n",
        "The purpose of the question is to practice different machine learning algorithms for **text clustering**.\n",
        "\n",
        "Please downlad the dataset by using the following link.  https://www.kaggle.com/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones\n",
        "(You can also use different text data which you want)\n",
        "\n",
        "**Apply the listed clustering methods to the dataset:**\n",
        "*   K-means\n",
        "*   DBSCAN\n",
        "*   Hierarchical clustering\n",
        "*   Word2Vec\n",
        "*   BERT\n",
        "\n",
        "You can refer to of the codes from  the follwing link below.\n",
        "https://www.kaggle.com/karthik3890/text-clustering"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentence-transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Es2bo3cu2QoZ",
        "outputId": "cab25f7d-251b-4b76-85f8-0ac4da6d2b48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.7.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.40.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('stsa_train.csv')\n",
        "print(data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FV45XeyBIBOd",
        "outputId": "3d910f0f-c1ac-4eaa-cc2b-379d7bdfc264"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   sentiment                                             review\n",
            "0          1  a stirring , funny and finally transporting re...\n",
            "1          0  apparently reassembled from the cutting-room f...\n",
            "2          0  they presume their audience wo n't sit still f...\n",
            "3          1  this is a visually stunning rumination on love...\n",
            "4          1  jonathan parker 's bartleby should have been t...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**K-means**"
      ],
      "metadata": {
        "id": "ALU0dnE2aoA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import pandas as pd\n",
        "\n",
        "# Feature extraction using TF-IDF\n",
        "tfidf = TfidfVectorizer(max_features=1000)\n",
        "X_features = tfidf.fit_transform(data['review'])\n",
        "\n",
        "# Determine the optimal number of clusters using silhouette score\n",
        "max_clusters = 10\n",
        "best_score = -1\n",
        "best_k = 2\n",
        "for k in range(2, max_clusters + 1):\n",
        "    kmeans = KMeans(n_clusters=k, n_init='auto', random_state=42)\n",
        "    kmeans.fit(X_features)\n",
        "    score = silhouette_score(X_features, kmeans.labels_)\n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        best_k = k\n",
        "\n",
        "# Perform k-means clustering with the optimal number of clusters\n",
        "kmeans = KMeans(n_clusters=best_k, n_init=10, random_state=42)\n",
        "kmeans.fit(X_features)\n",
        "\n",
        "# Assign cluster labels to each review\n",
        "data['kmeans_cluster'] = kmeans.labels_\n",
        "\n",
        "# Output the number of clusters and points in each cluster\n",
        "num_clusters = len(set(kmeans.labels_))\n",
        "print(f\"Number of clusters found: {num_clusters}\")\n",
        "print(\"Number of points in each cluster:\")\n",
        "print(data['kmeans_cluster'].value_counts())\n",
        "\n",
        "# Output the reviews along with their assigned clusters\n",
        "print(\"\\nReviews with cluster labels:\")\n",
        "print(data[['review', 'kmeans_cluster']])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wmyq10c2IOkr",
        "outputId": "2ffaa9f0-9b12-4109-acb1-67bb5c8a7849"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of clusters found: 10\n",
            "Number of points in each cluster:\n",
            "kmeans_cluster\n",
            "4    1552\n",
            "5    1036\n",
            "2     802\n",
            "3     792\n",
            "7     684\n",
            "0     569\n",
            "1     520\n",
            "6     388\n",
            "9     383\n",
            "8     194\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Reviews with cluster labels:\n",
            "                                                 review  kmeans_cluster\n",
            "0     a stirring , funny and finally transporting re...               2\n",
            "1     apparently reassembled from the cutting-room f...               4\n",
            "2     they presume their audience wo n't sit still f...               4\n",
            "3     this is a visually stunning rumination on love...               2\n",
            "4     jonathan parker 's bartleby should have been t...               5\n",
            "...                                                 ...             ...\n",
            "6915  painful , horrifying and oppressively tragic ,...               0\n",
            "6916  take care is nicely performed by a quintet of ...               3\n",
            "6917  the script covers huge , heavy topics in a bla...               4\n",
            "6918  a seriously bad film with seriously warped log...               0\n",
            "6919  a deliciously nonsensical comedy about a city ...               4\n",
            "\n",
            "[6920 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Output one review1 for each cluster\n",
        "print(\"Sample Reviews from Each Cluster:\")\n",
        "for cluster_label in range(best_k):\n",
        "    cluster_reviews = data[data['kmeans_cluster'] == cluster_label]['review'].head(1)\n",
        "    print(f\"\\nCluster {cluster_label}:\")\n",
        "    for review in cluster_reviews:\n",
        "        print(review)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwgTT61_ZkH6",
        "outputId": "b61fb7bd-830a-411a-d8ca-99a9ae9e4917"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Reviews from Each Cluster:\n",
            "\n",
            "Cluster 0:\n",
            "a fan film that for the uninitiated plays better on video with the sound turned down .\n",
            "\n",
            "Cluster 1:\n",
            "a wretched movie that reduces the second world war to one man 's quest to find an old flame .\n",
            "\n",
            "Cluster 2:\n",
            "a stirring , funny and finally transporting re-imagining of beauty and the beast and 1930s horror films\n",
            "\n",
            "Cluster 3:\n",
            "may be more genial than ingenious , but it gets the job done .\n",
            "\n",
            "Cluster 4:\n",
            "apparently reassembled from the cutting-room floor of any given daytime soap .\n",
            "\n",
            "Cluster 5:\n",
            "jonathan parker 's bartleby should have been the be-all-end-all of the modern-office anomie films .\n",
            "\n",
            "Cluster 6:\n",
            "for something as splendid-looking as this particular film , the viewer expects something special but instead gets -lrb- sci-fi -rrb- rehash .\n",
            "\n",
            "Cluster 7:\n",
            "bÃ©art and berling are both superb , while huppert ... is magnificent .\n",
            "\n",
            "Cluster 8:\n",
            "blue crush follows the formula , but throws in too many conflicts to keep the story compelling .\n",
            "\n",
            "Cluster 9:\n",
            "final verdict : you 've seen it all before .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DBSCAN**"
      ],
      "metadata": {
        "id": "uM-zJeL9a0aA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
        "\n",
        "# Fit and transform the text data to TF-IDF vectors\n",
        "tfidf_vectors = tfidf_vectorizer.fit_transform(data['review'])\n",
        "\n",
        "# Determine the optimal epsilon value using silhouette score\n",
        "best_eps = None\n",
        "best_score = -1\n",
        "for eps in [0.1, 0.5, 1.0, 1.5, 2.0]:\n",
        "    dbscan = DBSCAN(eps=eps, min_samples=5)\n",
        "    dbscan.fit(tfidf_vectors)\n",
        "    unique_labels = len(set(dbscan.labels_)) - (1 if -1 in dbscan.labels_ else 0)  # Exclude noise points (-1)\n",
        "    if unique_labels > 1:\n",
        "        score = silhouette_score(tfidf_vectors, dbscan.labels_)\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_eps = eps\n",
        "\n",
        "# Perform DBSCAN clustering with the optimal epsilon value\n",
        "dbscan = DBSCAN(eps=best_eps, min_samples=5)\n",
        "dbscan.fit(tfidf_vectors)\n",
        "\n",
        "# Assign cluster labels to each review\n",
        "data['dbscan_cluster'] = dbscan.labels_\n",
        "\n",
        "# Output the number of clusters and points in each cluster\n",
        "num_clusters = len(set(dbscan.labels_)) - (1 if -1 in dbscan.labels_ else 0)  # Exclude noise points (-1)\n",
        "print(f\"Number of clusters found: {num_clusters}\")\n",
        "print(\"Number of points in each cluster:\")\n",
        "print(data['dbscan_cluster'].value_counts())\n",
        "\n",
        "# Output the reviews along with their assigned clusters\n",
        "print(\"\\nReviews with cluster labels:\")\n",
        "print(data[['review', 'dbscan_cluster']])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4fe9U4tpwOu",
        "outputId": "04cbbe74-b281-4ed0-f5c9-23b7aec1ce0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of clusters found: 4\n",
            "Number of points in each cluster:\n",
            "dbscan_cluster\n",
            "-1    6837\n",
            " 0      57\n",
            " 2      14\n",
            " 3       7\n",
            " 1       5\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Reviews with cluster labels:\n",
            "                                                 review  dbscan_cluster\n",
            "0     a stirring , funny and finally transporting re...              -1\n",
            "1     apparently reassembled from the cutting-room f...              -1\n",
            "2     they presume their audience wo n't sit still f...              -1\n",
            "3     this is a visually stunning rumination on love...              -1\n",
            "4     jonathan parker 's bartleby should have been t...              -1\n",
            "...                                                 ...             ...\n",
            "6915  painful , horrifying and oppressively tragic ,...              -1\n",
            "6916  take care is nicely performed by a quintet of ...              -1\n",
            "6917  the script covers huge , heavy topics in a bla...              -1\n",
            "6918  a seriously bad film with seriously warped log...              -1\n",
            "6919  a deliciously nonsensical comedy about a city ...              -1\n",
            "\n",
            "[6920 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Output one review for each cluster\n",
        "print(\"\\nOne review for each cluster:\")\n",
        "for cluster_label in sorted(set(dbscan.labels_)):\n",
        "    if cluster_label == -1:\n",
        "        continue\n",
        "    cluster_reviews = data[data['dbscan_cluster'] == cluster_label]['review'].head(1)\n",
        "    print(f\"Cluster {cluster_label}:\")\n",
        "    for review in cluster_reviews:\n",
        "        print(f\"  - {review}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsMRnQ7uYU21",
        "outputId": "0f601149-d263-4512-e74b-bd660c7e4500"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "One review for each cluster:\n",
            "Cluster 0:\n",
            "  - amazingly lame .\n",
            "Cluster 1:\n",
            "  - fun and nimble .\n",
            "Cluster 2:\n",
            "  - technically and artistically inept .\n",
            "Cluster 3:\n",
            "  - remember it .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hierarchical clustering**"
      ],
      "metadata": {
        "id": "vHgudxJka9wv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# Convert sparse matrix to dense array\n",
        "X_dense = X_features.toarray()\n",
        "\n",
        "# Perform hierarchical clustering\n",
        "agglomerative = AgglomerativeClustering(n_clusters=2)\n",
        "agglomerative.fit(X_dense)\n",
        "\n",
        "# Assign cluster labels to each review\n",
        "data['agglomerative_cluster'] = agglomerative.labels_\n",
        "\n",
        "# Output the reviews along with their assigned clusters\n",
        "print(\"Reviews with cluster labels:\")\n",
        "print(data[['review', 'agglomerative_cluster']])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tE1qC-UO4Uz",
        "outputId": "e43cc574-50e5-4ba7-f6e0-13b3922e5340"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reviews with cluster labels:\n",
            "                                                 review  agglomerative_cluster\n",
            "0     a stirring , funny and finally transporting re...                      0\n",
            "1     apparently reassembled from the cutting-room f...                      1\n",
            "2     they presume their audience wo n't sit still f...                      0\n",
            "3     this is a visually stunning rumination on love...                      0\n",
            "4     jonathan parker 's bartleby should have been t...                      0\n",
            "...                                                 ...                    ...\n",
            "6915  painful , horrifying and oppressively tragic ,...                      0\n",
            "6916  take care is nicely performed by a quintet of ...                      0\n",
            "6917  the script covers huge , heavy topics in a bla...                      0\n",
            "6918  a seriously bad film with seriously warped log...                      0\n",
            "6919  a deliciously nonsensical comedy about a city ...                      0\n",
            "\n",
            "[6920 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the data by agglomerative cluster labels\n",
        "cluster_groups = data.groupby('agglomerative_cluster')\n",
        "\n",
        "# Print two reviews for each cluster\n",
        "for cluster_label, reviews_group in cluster_groups:\n",
        "    print(f\"Cluster {cluster_label}:\")\n",
        "    reviews = reviews_group['review'].head(2)\n",
        "    for review in reviews:\n",
        "        print(review)\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UtlwCK0Y-lj",
        "outputId": "880b56ee-d592-4ed8-a3c7-4806d1a8a064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster 0:\n",
            "a stirring , funny and finally transporting re-imagining of beauty and the beast and 1930s horror films\n",
            "they presume their audience wo n't sit still for a sociology lesson , however entertainingly presented , so they trot out the conventional science-fiction elements of bug-eyed monsters and futuristic women in skimpy clothes .\n",
            "\n",
            "Cluster 1:\n",
            "apparently reassembled from the cutting-room floor of any given daytime soap .\n",
            "it is as uncompromising as it is nonjudgmental , and makes clear that a prostitute can be as lonely and needy as any of the clients .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word2Vec**"
      ],
      "metadata": {
        "id": "SzKYvTcHbGQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Train Word2Vec model\n",
        "sentences = [review.split() for review in data['review']]\n",
        "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Generate embeddings for each review\n",
        "word2vec_features = []\n",
        "for review in sentences:\n",
        "    embeddings = [word2vec_model.wv[word] for word in review if word in word2vec_model.wv]\n",
        "    if embeddings:\n",
        "        review_embedding = sum(embeddings) / len(embeddings)\n",
        "        word2vec_features.append(review_embedding)\n",
        "    else:\n",
        "        word2vec_features.append([0] * 100)\n",
        "\n",
        "# Determine the optimal number of clusters for K-means clustering\n",
        "max_clusters = 10\n",
        "best_score = -1\n",
        "best_n_clusters = 2\n",
        "for n_clusters in range(2, max_clusters + 1):\n",
        "    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
        "    kmeans.fit(word2vec_features)\n",
        "    score = silhouette_score(word2vec_features, kmeans.labels_)\n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        best_n_clusters = n_clusters\n",
        "\n",
        "# Perform K-means clustering with the optimal number of clusters\n",
        "kmeans = KMeans(n_clusters=best_n_clusters, n_init=10, random_state=42)\n",
        "kmeans.fit(word2vec_features)\n",
        "data['word2vec_cluster'] = kmeans.labels_\n",
        "\n",
        "# Output the number of points in each cluster\n",
        "print(\"Number of points in each cluster:\")\n",
        "print(data['word2vec_cluster'].value_counts())\n",
        "\n",
        "# Output the reviews along with their assigned clusters\n",
        "print(\"\\nWord2Vec clustering:\")\n",
        "print(data[['review', 'word2vec_cluster']])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vY0Ww3oBIjNF",
        "outputId": "f60a1086-795f-4b40-e7fc-bcf8aedb83f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of points in each cluster:\n",
            "word2vec_cluster\n",
            "1    3633\n",
            "0    3287\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Word2Vec clustering:\n",
            "                                                 review  word2vec_cluster\n",
            "0     a stirring , funny and finally transporting re...                 0\n",
            "1     apparently reassembled from the cutting-room f...                 0\n",
            "2     they presume their audience wo n't sit still f...                 0\n",
            "3     this is a visually stunning rumination on love...                 1\n",
            "4     jonathan parker 's bartleby should have been t...                 0\n",
            "...                                                 ...               ...\n",
            "6915  painful , horrifying and oppressively tragic ,...                 1\n",
            "6916  take care is nicely performed by a quintet of ...                 0\n",
            "6917  the script covers huge , heavy topics in a bla...                 1\n",
            "6918  a seriously bad film with seriously warped log...                 0\n",
            "6919  a deliciously nonsensical comedy about a city ...                 0\n",
            "\n",
            "[6920 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print one review for each Word2Vec cluster\n",
        "print(\"\\nOne review for each Word2Vec cluster:\")\n",
        "for cluster_id in range(best_n_clusters):\n",
        "    cluster_reviews = data[data['word2vec_cluster'] == cluster_id]['review'][:1]  # Get the first two reviews\n",
        "    print(f\"\\nCluster {cluster_id}:\\n\")\n",
        "    for review in cluster_reviews:\n",
        "        print(review)\n",
        "        print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4tUvpoBR5KL",
        "outputId": "a1823508-2fd3-40ba-e689-9641371ad1e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "One review for each Word2Vec cluster:\n",
            "\n",
            "Cluster 0:\n",
            "\n",
            "a stirring , funny and finally transporting re-imagining of beauty and the beast and 1930s horror films\n",
            "\n",
            "\n",
            "Cluster 1:\n",
            "\n",
            "this is a visually stunning rumination on love , memory , history and the war between art and commerce .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BERT**"
      ],
      "metadata": {
        "id": "6J-1ODcFbMzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Load pre-trained BERT model\n",
        "bert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "# Encode text reviews using BERT\n",
        "review_embeddings = bert_model.encode(data['review'].tolist())\n",
        "\n",
        "# Determine the optimal number of clusters for K-means clustering\n",
        "max_clusters = 20\n",
        "best_score = -1\n",
        "best_n_clusters = 2\n",
        "for n_clusters in range(2, max_clusters + 1):\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    kmeans.fit(review_embeddings)\n",
        "    score = silhouette_score(review_embeddings, kmeans.labels_)\n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        best_n_clusters = n_clusters\n",
        "\n",
        "# Perform K-means clustering with the optimal number of clusters\n",
        "kmeans = KMeans(n_clusters=best_n_clusters, random_state=42)\n",
        "kmeans.fit(review_embeddings)\n",
        "data['bert_cluster'] = kmeans.labels_\n",
        "\n",
        "# Print the number of points in each cluster\n",
        "print(\"Number of points in each cluster:\")\n",
        "print(data['bert_cluster'].value_counts())\n",
        "\n",
        "# Print the output\n",
        "print(\"\\nBERT clustering:\")\n",
        "print(data[['review', 'bert_cluster']])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VndoOzKNIlMm",
        "outputId": "711057a0-72ea-42cc-e085-ff4e5147d170"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of points in each cluster:\n",
            "bert_cluster\n",
            "1    3905\n",
            "0    3015\n",
            "Name: count, dtype: int64\n",
            "\n",
            "BERT clustering:\n",
            "                                                 review  bert_cluster\n",
            "0     a stirring , funny and finally transporting re...             0\n",
            "1     apparently reassembled from the cutting-room f...             1\n",
            "2     they presume their audience wo n't sit still f...             1\n",
            "3     this is a visually stunning rumination on love...             0\n",
            "4     jonathan parker 's bartleby should have been t...             1\n",
            "...                                                 ...           ...\n",
            "6915  painful , horrifying and oppressively tragic ,...             1\n",
            "6916  take care is nicely performed by a quintet of ...             0\n",
            "6917  the script covers huge , heavy topics in a bla...             1\n",
            "6918  a seriously bad film with seriously warped log...             1\n",
            "6919  a deliciously nonsensical comedy about a city ...             1\n",
            "\n",
            "[6920 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print two reviews from each BERT cluster\n",
        "print(\"\\nTwo reviews from each BERT cluster:\")\n",
        "best_n_clusters = 2\n",
        "for cluster_id in range(best_n_clusters):\n",
        "    cluster_reviews = data[data['bert_cluster'] == cluster_id]['review'][:2]  # Get the first two reviews\n",
        "    print(f\"\\nCluster {cluster_id}:\\n\")\n",
        "    for review in cluster_reviews:\n",
        "        print(review)\n",
        "        print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9Ue7umCS5p-",
        "outputId": "f360aaac-39f3-4f95-8d52-b0a3f629fff0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Two reviews from each BERT cluster:\n",
            "\n",
            "Cluster 0:\n",
            "\n",
            "a stirring , funny and finally transporting re-imagining of beauty and the beast and 1930s horror films\n",
            "\n",
            "this is a visually stunning rumination on love , memory , history and the war between art and commerce .\n",
            "\n",
            "\n",
            "Cluster 1:\n",
            "\n",
            "apparently reassembled from the cutting-room floor of any given daytime soap .\n",
            "\n",
            "they presume their audience wo n't sit still for a sociology lesson , however entertainingly presented , so they trot out the conventional science-fiction elements of bug-eyed monsters and futuristic women in skimpy clothes .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "boXmyRKzn-pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In one paragraph, please compare the results of K-means, DBSCAN, Hierarchical clustering, Word2Vec, and BERT.**"
      ],
      "metadata": {
        "id": "tRijW2aLGONl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The clustering algorithm produced to different results. K-means algorithm divided reviewers into 10 groups, while the DBSCAN used only 4 groups which majority are marked as outliers. Hierachical clustering doesn't state the number of clusters as crisply. Word2Vec and BERT resulted in 2 clusters each BERT clusters showed are more even than Word2Vec clusters. You will need to appraise these result via domain-specific criteria to finally figure out which method is able to display a specific context within your review data."
      ],
      "metadata": {
        "id": "pIYCj5qyGfSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment.\n",
        "\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "In this assignment, I saw how various clustering techniques and text embedding methods were applied, and,\n",
        "though each of them has its particular set of benefits and drawbacks, we can surely see that they offer a very efficient means\n",
        " to organize and interpret raw data. With K-means clustering, I noticed that it has a clean interface,very suitable for the\n",
        " data that can be well-separated.\n",
        " DBSCAN was maybe the one that caught my attention during the presentation of\n",
        " its ability to identify the outliers automatically and adapt to different shaped clusters which could rely on different sizes.\n",
        "the BERT expansion has supplied me with many paragraph like representations of text, but I had to deal with the high\n",
        "computational costs and preprocessing requirements inherent to this method as well.\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}